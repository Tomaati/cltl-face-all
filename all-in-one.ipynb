{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361264bitdevpython365fea5171f5014fceae2f90894565cfd8",
   "display_name": "Python 3.6.12 64-bit ('dev-python-3.6')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[*] load ckpt from /home/tk/repos/cltl-face-all/arcface/./pretrained_models/arc_res50/e_8_b_40000.ckpt\n",
      "random dialogue : dia474\n",
      "\n",
      "number of utterance videos is 19\n",
      "['/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt0.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt1.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt10.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt11.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt12.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt13.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt14.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt15.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt16.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt17.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt18.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt2.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt3.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt4.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt5.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt6.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt7.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt8.mp4', '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt9.mp4']\n",
      "\n",
      "random utt: /home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia474_utt14.mp4\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-95b105eb63a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "from face_alignment import FaceDetection\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "from arcface import ArcFace\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2\n",
    "from agegender import AgeGender\n",
    "\n",
    "\n",
    "# HERE = os.path.dirname(os.path.abspath(__file__))\n",
    "HERE = os.getcwd()\n",
    "\n",
    "ag = AgeGender()\n",
    "af = ArcFace()\n",
    "fd = FaceDetection(face_detector='blazeface')\n",
    "\n",
    "with open('/home/tk/datasets/MELD/MELD.Raw/train/small_dataset.json', 'r') as stream:\n",
    "    small_dataset = json.load(stream)\n",
    "\n",
    "vid_root_path = '/home/tk/datasets/MELD/MELD.Raw/train/train_splits/'\n",
    "\n",
    "random_dia = random.choice(small_dataset['train'])\n",
    "print(f\"random dialogue : {random_dia}\")\n",
    "print()\n",
    "\n",
    "utts = glob(vid_root_path + random_dia + '*.mp4')\n",
    "utts.sort()\n",
    "print(f\"number of utterance videos is {len(utts)}\")\n",
    "print(utts)\n",
    "print()\n",
    "\n",
    "utt = random.choice(utts)\n",
    "print(f\"random utt: {utt}\")\n",
    "print()\n",
    "\n",
    "cap = cv2.VideoCapture(utt)\n",
    "frames = []\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frames.append(frame)\n",
    "\n",
    "print(f\"number of frames in this utt is {len(frames)}\")\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "num_iter = len(frames) // batch_size\n",
    "remainders = len(frames) % batch_size\n",
    "landmarks = []\n",
    "bboxes = []\n",
    "faces = []\n",
    "\n",
    "for i in tqdm(range(num_iter)):\n",
    "    batch = np.stack(frames[i*batch_size: (i+1)*batch_size])\n",
    "    print(batch.shape)\n",
    "\n",
    "    bboxes_ = fd.detect_faces(batch)\n",
    "    landmarks_ = fd.detect_landmarks(batch, bboxes_)\n",
    "    faces_ = fd.crop_and_align(batch, bboxes_, landmarks_)\n",
    "    for bb, lm, fa in zip(bboxes_, landmarks_, faces_):\n",
    "        landmarks.append(lm)\n",
    "        bboxes.append(bb)\n",
    "        faces.append(fa)\n",
    "\n",
    "if remainders != 0:\n",
    "    batch = np.stack(frames[num_iter*batch_size:])\n",
    "    print(batch.shape)\n",
    "\n",
    "    bboxes_ = fd.detect_faces(batch)\n",
    "    landmarks_ = fd.detect_landmarks(batch, bboxes_)\n",
    "    faces_ = fd.crop_and_align(batch, bboxes_, landmarks_)\n",
    "\n",
    "    for bb, lm, fa in zip(bboxes_, landmarks_, faces_):\n",
    "        landmarks.append(lm)\n",
    "        bboxes.append(bb)\n",
    "        faces.append(fa)\n",
    "\n",
    "num_real_bboxes = [len(bb) for bb in bboxes]\n",
    "num_real_landmarks = [len(lm) for lm in landmarks]\n",
    "num_real_faces = [len(fc) for fc in faces]\n",
    "\n",
    "\n",
    "assert num_real_bboxes == num_real_landmarks == num_real_faces\n",
    "\n",
    "num_real = num_real_bboxes\n",
    "\n",
    "bboxes = np.concatenate(bboxes)\n",
    "landmarks = np.concatenate(landmarks)\n",
    "faces = np.concatenate(faces)\n",
    "\n",
    "print(bboxes.shape, landmarks.shape, faces.shape)\n",
    "\n",
    "age, gender = ag.predict(faces)\n",
    "print(age, gender)\n",
    "\n",
    "embeddings = af.predict(images)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}