{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we use the [MELD dataset](https://affective-meld.github.io/) to extract videos and faces.\n",
    "\n",
    "Download [this small_dataset json](https://raw.githubusercontent.com/cltl/ma-communicative-robots/master/multimodal/small_dataset.json), as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ben 1\n",
      "Chandler 5\n",
      "Eric 1\n",
      "Janice 1\n",
      "Joey 7\n",
      "Man 1\n",
      "Monica 14\n",
      "Mr. Franklin 1\n",
      "Mr. Treeger 1\n",
      "Phoebe 5\n",
      "Rachel 5\n",
      "Rick 1\n",
      "Ross 6\n",
      "Terry 1\n",
      "\n",
      "The number of videos BEFORE removing not important name videos: 50\n",
      "The number of videos AFTER removing not important name videos: 50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "\n",
    "raw_train_videos_path = '/home/tk/datasets/MELD/MELD.Raw/train/train_splits'\n",
    "train_sent_emo_path = '/home/tk/repos/MELD/data/MELD/train_sent_emo.csv'\n",
    "small_dataset_path = '/home/tk/datasets/MELD/MELD.Raw/train/small_dataset.json'\n",
    "save_dir = '/home/tk/repos/cltl-face-all/your-faces'\n",
    "\n",
    "with open(small_dataset_path, 'r') as stream:\n",
    "    small_dataset = json.load(stream)\n",
    "\n",
    "small_dataset = sorted(small_dataset['train'])\n",
    "\n",
    "with open(train_sent_emo_path) as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "\n",
    "dia_utts = []\n",
    "for d in data:\n",
    "    name = d[2]\n",
    "    did = d[5]\n",
    "    uid = d[6]\n",
    "    vidfile = f\"dia{did}_utt{uid}.mp4\"\n",
    "\n",
    "    dia = f\"dia{did}\"\n",
    "\n",
    "    if dia in small_dataset:\n",
    "        dia_utts.append((vidfile, name))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "random.shuffle(dia_utts)\n",
    "dia_utts = dia_utts[:50]\n",
    "\n",
    "names_mentioned = [name for du, name in dia_utts]\n",
    "unique_names = sorted(list(set(names_mentioned)))\n",
    "important_names = []\n",
    "\n",
    "for un in unique_names:\n",
    "    num_occurences = names_mentioned.count(un)\n",
    "#     if num_occurences > 5:\n",
    "    print(un, num_occurences)\n",
    "    important_names.append(un)\n",
    "print()\n",
    "\n",
    "print(f\"The number of videos BEFORE removing not important name videos: {len(dia_utts)}\")\n",
    "\n",
    "for du_name in list(dia_utts):\n",
    "    du, name = du_name\n",
    "    if name not in important_names:\n",
    "        dia_utts.remove(du_name)\n",
    "\n",
    "dia_utts.sort(key=lambda x:x[1])\n",
    "\n",
    "print(f\"The number of videos AFTER removing not important name videos: {len(dia_utts)}\")\n",
    "\n",
    "for name in important_names:\n",
    "    shutil.rmtree(os.path.join(save_dir, name), ignore_errors=True)\n",
    "    os.mkdir(os.path.join(save_dir, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] load ckpt from /home/tk/.virtualenvs/dev-python-3.7/lib/python3.7/site-packages/cltl_face_all/arcface/./pretrained_models/arc_res50/e_8_b_40000.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738f0073136b4374af7b3fd3b7e4c411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia500_utt1.mp4\n",
      "37\n",
      "(37, 720, 1280, 3)\n",
      "37\n",
      "37\n",
      "(59, 112, 112, 3)\n",
      "(59, 512)\n",
      "\n",
      "/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia664_utt8.mp4\n",
      "64\n",
      "(64, 720, 1280, 3)\n",
      "64\n",
      "64\n",
      "(173, 112, 112, 3)\n",
      "(173, 512)\n",
      "11\n",
      "(11, 720, 1280, 3)\n",
      "11\n",
      "11\n",
      "(23, 112, 112, 3)\n",
      "(23, 512)\n",
      "\n",
      "/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia332_utt2.mp4\n",
      "43\n",
      "(43, 720, 1280, 3)\n",
      "43\n",
      "43\n",
      "(102, 112, 112, 3)\n",
      "(102, 512)\n",
      "\n",
      "/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia274_utt0.mp4\n",
      "64\n",
      "(64, 720, 1280, 3)\n",
      "64\n",
      "64\n",
      "(32, 112, 112, 3)\n",
      "(32, 512)\n",
      "64\n",
      "(64, 720, 1280, 3)\n",
      "64\n",
      "64\n",
      "(17, 112, 112, 3)\n",
      "(17, 512)\n",
      "32\n",
      "(32, 720, 1280, 3)\n",
      "32\n",
      "32\n",
      "(20, 112, 112, 3)\n",
      "(20, 512)\n",
      "\n",
      "/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia822_utt7.mp4\n",
      "23\n",
      "(23, 720, 1280, 3)\n",
      "23\n",
      "23\n",
      "(48, 112, 112, 3)\n",
      "(48, 512)\n",
      "\n",
      "/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia616_utt2.mp4\n",
      "64\n",
      "(64, 720, 1280, 3)\n",
      "64\n",
      "64\n",
      "(78, 112, 112, 3)\n",
      "(78, 512)\n",
      "19\n",
      "(19, 720, 1280, 3)\n",
      "19\n",
      "19\n",
      "(39, 112, 112, 3)\n",
      "(39, 512)\n",
      "\n",
      "/home/tk/datasets/MELD/MELD.Raw/train/train_splits/dia884_utt4.mp4\n",
      "55\n",
      "(55, 720, 1280, 3)\n",
      "55\n",
      "55\n",
      "(66, 112, 112, 3)\n",
      "(66, 512)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "from cltl_face_all.agegender import AgeGender\n",
    "from cltl_face_all.arcface import ArcFace\n",
    "from cltl_face_all.arcface import calc_angle_distance\n",
    "from cltl_face_all.face_alignment import FaceDetection\n",
    "import uuid\n",
    "\n",
    "\n",
    "ag = AgeGender(device='cpu')\n",
    "af = ArcFace(device='cpu')\n",
    "fd = FaceDetection(device='cpu', face_detector='blazeface')\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "face_threshold = 0.9\n",
    "\n",
    "\n",
    "for du, name in tqdm(dia_utts):\n",
    "    full_video_path = os.path.join(raw_train_videos_path, du)\n",
    "\n",
    "    container = av.open(full_video_path)\n",
    "\n",
    "    batch = []\n",
    "    print(full_video_path)\n",
    "    for frame in container.decode(video=0):\n",
    "        idx = frame.index\n",
    "        numpy_RGB = np.array(frame.to_image())\n",
    "        batch.append(numpy_RGB)\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            print(len(batch))\n",
    "\n",
    "            batch = np.stack(batch, axis=0)\n",
    "            print(batch.shape)\n",
    "            bboxes = fd.detect_faces(batch)\n",
    "            print(len(bboxes))\n",
    "            landmarks = fd.detect_landmarks(batch, bboxes)\n",
    "            faces = fd.crop_and_align(batch, bboxes, landmarks)\n",
    "            print(len(faces))\n",
    "            faces = np.concatenate(faces, axis=0)\n",
    "            print(faces.shape)\n",
    "            embeddings = af.predict(faces)\n",
    "            print(embeddings.shape)\n",
    "\n",
    "            assert len(faces) == len(embeddings)\n",
    "\n",
    "            for bb, fa, em in zip(bboxes, faces, embeddings):\n",
    "                if len(bb) == 0:\n",
    "                    continue\n",
    "                prob = bb[0][4]\n",
    "                if prob > face_threshold:\n",
    "                    unique_name = str(uuid.uuid4())\n",
    "                    cv2.imwrite(os.path.join(save_dir, name, unique_name) + '.jpg', cv2.cvtColor(fa, cv2.COLOR_RGB2BGR))\n",
    "                    with open(os.path.join(save_dir, name, unique_name) + '.npy', 'wb') as stream:\n",
    "                        np.save(stream, em)\n",
    "\n",
    "\n",
    "            batch = []\n",
    "    if len(batch) != 0:\n",
    "        print(len(batch))\n",
    "\n",
    "        batch = np.stack(batch, axis=0)\n",
    "        print(batch.shape)\n",
    "        bboxes = fd.detect_faces(batch)\n",
    "        print(len(bboxes))\n",
    "        landmarks = fd.detect_landmarks(batch, bboxes)\n",
    "        faces = fd.crop_and_align(batch, bboxes, landmarks)\n",
    "        print(len(faces))\n",
    "        faces = np.concatenate(faces, axis=0)\n",
    "        print(faces.shape)\n",
    "        embeddings = af.predict(faces)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        assert len(faces) == len(embeddings)\n",
    "\n",
    "        for bb, fa, em in zip(bboxes, faces, embeddings):\n",
    "            if len(bb) == 0:\n",
    "                continue\n",
    "\n",
    "            prob = bb[0][4]\n",
    "            if prob > face_threshold:\n",
    "                unique_name = str(uuid.uuid4())\n",
    "                cv2.imwrite(os.path.join(save_dir, name, unique_name) + '.jpg', cv2.cvtColor(fa, cv2.COLOR_RGB2BGR))\n",
    "                unique_name = str(uuid.uuid4())\n",
    "                with open(os.path.join(save_dir, name, unique_name) + '.npy', 'wb') as stream:\n",
    "                    np.save(stream, em)\n",
    "\n",
    "        batch = []\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now go through the images and delete the wrong images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "important_names = ['Chandler', 'Joey', 'Monica', 'Phoebe', 'Rachel', 'Ross']\n",
    "for name in important_names:\n",
    "    npys = glob(os.path.join(save_dir, name, \"*.npy\"))\n",
    "\n",
    "    for npy in list(npys):\n",
    "        jpg = npy.replace('.npy', '.jpg')\n",
    "        if not os.path.isfile(jpg):\n",
    "            os.remove(npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joey\n",
      "(11, 512)\n",
      "[[0.   0.72 0.42 0.41 0.56 0.66 0.47 0.56 0.42 0.41 0.48]\n",
      " [0.72 0.   0.6  0.62 0.66 0.24 0.61 0.31 0.52 0.55 0.67]\n",
      " [0.42 0.6  0.   0.44 0.51 0.55 0.47 0.47 0.26 0.4  0.24]\n",
      " [0.41 0.62 0.44 0.   0.46 0.55 0.19 0.47 0.46 0.47 0.49]\n",
      " [0.56 0.66 0.51 0.46 0.   0.61 0.5  0.58 0.52 0.56 0.59]\n",
      " [0.66 0.24 0.55 0.55 0.61 0.   0.53 0.23 0.45 0.48 0.61]\n",
      " [0.47 0.61 0.47 0.19 0.5  0.53 0.   0.48 0.5  0.52 0.53]\n",
      " [0.56 0.31 0.47 0.47 0.58 0.23 0.48 0.   0.35 0.39 0.53]\n",
      " [0.42 0.52 0.26 0.46 0.52 0.45 0.5  0.35 0.   0.28 0.38]\n",
      " [0.41 0.55 0.4  0.47 0.56 0.48 0.52 0.39 0.28 0.   0.47]\n",
      " [0.48 0.67 0.24 0.49 0.59 0.61 0.53 0.53 0.38 0.47 0.  ]]\n",
      "\n",
      "[8 2 7 9 3]\n",
      "5\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "from cltl_face_all.arcface import calc_angle_distance\n",
    "\n",
    "for name in important_names:\n",
    "    print(name)\n",
    "    npys = glob(os.path.join(save_dir, name, \"*.npy\"))\n",
    "    embs = []\n",
    "\n",
    "    for npy in npys:\n",
    "        with open(npy, 'rb') as stream:\n",
    "            embs.append(np.load(stream))\n",
    "    embs = np.stack(embs)\n",
    "    print(embs.shape)\n",
    "\n",
    "    dists = calc_angle_distance(embs, embs)\n",
    "    print(dists.round(2))\n",
    "    print()\n",
    "\n",
    "    indexes = dists.mean(axis=1).argsort()[:len(dists.mean(axis=1)) // 2]\n",
    "\n",
    "    print(indexes)\n",
    "    embs = embs[indexes]\n",
    "    print(len(embs))\n",
    "\n",
    "\n",
    "    emb_mean = embs.mean(axis=0)\n",
    "    emb_final = emb_mean / np.linalg.norm(emb_mean)\n",
    "    print(emb_final.shape)\n",
    "\n",
    "    with open(os.path.join(save_dir, name) + '.npy', 'wb') as stream:\n",
    "        np.save(stream, emb_final)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
